{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "#File_path = '/home/linu/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "no_of_recommends = 20\n",
    "n_topics = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Topic_Modeller(LDA_matrix):\n",
    "    \n",
    "    total_WordVocab = []\n",
    "    for i in range(0,len(cleaned_articles)) :\n",
    "        word_tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "        for words in word_tokens :\n",
    "            total_WordVocab.append(words)\n",
    "        counts = Counter(total_WordVocab)\n",
    "\n",
    "    vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "    stops_removed = [word for word in vocab.keys() if word not in stops]\n",
    "\n",
    "    Final_VocabDict = {j:i for i,j in enumerate(stops_removed)}\n",
    "    \n",
    "    Tfidf = TfidfVectorizer(min_df=1,vocabulary=Final_VocabDict)\n",
    "\n",
    "\n",
    "    Tfidf_Matrix = Tfidf.fit_transform(cleaned_articles)\n",
    "\n",
    "    Lda = LatentDirichletAllocation(n_components=10,max_iter=1,random_state=0)\n",
    "    \n",
    "    Lda_articlemat = Lda.fit_transform(Tfidf_Matrix)\n",
    "\n",
    "    return Lda_articlemat  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lda_articlemat = Topic_Modeller(cleaned_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 10)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lda_articlemat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtokens_article = [word.split() for word in cleaned_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to create user profiles on the base of articles read and time spent  for implementing content approach\n",
    "\n",
    "def user_profiler(wordtokens,article_read,article_time):\n",
    "    user_profile = []\n",
    "    wordPer_second = 5\n",
    "    \n",
    "\n",
    "    for i in range(len(wordtokens)):                                        \n",
    "\n",
    "        average_time = (len(wordtokens[i])/wordPer_second) #length of wordtokslist by wps gives us avg time to read the article\n",
    "         \n",
    "        user_interest_timevalue = article_time[i]/average_time  #article_times divide by avg times of each article                   \n",
    "        \n",
    "        user_profile_generate = (article_read[i]*user_interest_timevalue)   #Ldamatrix[] * user_interest_time calculated                 \n",
    "        \n",
    "        user_profile.append(user_profile_generate)                                      \n",
    "\n",
    "    return sum(normalize(user_profile))                             \n",
    "\n",
    "\n",
    "userProfile_One = user_profiler([wordtokens_article[600],wordtokens_article[99],wordtokens_article[120]],\n",
    "                             [Lda_articlemat[600],Lda_articlemat[99],Lda_articlemat[120]],\n",
    "                             [120,60,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "userprofile_one =  np.array(userProfile_One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userprofile_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06380809, 1.03563938, 0.06380688, 1.05161394, 0.06380264,\n",
       "       0.0638015 , 1.03360147, 0.06380235, 0.06380346, 0.06380215])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userprofile_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Content_Recommends_Calculator(user_profile,Lda_articlemat) :\n",
    "    \n",
    "    user_interested_articles = []\n",
    "    \n",
    "    contents_interest_score = []\n",
    "\n",
    "    user_preffered_articles = cosine_similarity(userprofile_one.reshape(1,-1),Lda_articlemat)\n",
    "    \n",
    "    top_articles = np.sort(user_preffered_articles).flatten()[::-1][:10]\n",
    "    \n",
    "    user_interested_articles.append(top_articles)\n",
    "    \n",
    "    content_interest_score = (user_interested_articles[0] * 0.4)\n",
    "    \n",
    "    return content_interest_score\n",
    "\n",
    "\n",
    "content_recommended = Content_Recommends_Calculator(userprofile_one,Lda_articlemat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33218085, 0.33066672, 0.3303857 , 0.33036246, 0.32982406,\n",
       "       0.32849261, 0.3283279 , 0.3282375 , 0.32813684, 0.32796289])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For CF apporach\n",
    "existing_users = np.random.random_sample(size=(1000,10))   #we take n existing users   \n",
    "new_user = np.random.random_sample(size=(1,10))            #we take a single new user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Collaborative_Recommends_Calculator(existing_usr,new_usr) :\n",
    "    \n",
    "    collaborative_interest_score = [ ]\n",
    "    sorted_collaborative_interest = [ ]\n",
    "    \n",
    "    collaborative_interest_score = cosine_similarity(existing_users,new_user)\n",
    "    \n",
    "    sorted_collaborative_interest = np.argsort(collaborative_interest_score,axis=0)[::-1][:10]\n",
    "    \n",
    "    sorted_collaborative_indexes = existing_users[sorted_collaborative_interest]\n",
    "    \n",
    "    collab_interest = np.mean(sorted_collaborative_indexes.reshape(-1,10),axis=0)\n",
    "    \n",
    "    collaborative_interest_scores = collab_interest*0.6\n",
    "    \n",
    "    return collaborative_interest_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "collab_recommended = Collaborative_Recommends_Calculator(existing_users,new_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17969108, 0.29012024, 0.09446668, 0.51803656, 0.17033427,\n",
       "       0.4189131 , 0.39565082, 0.1548523 , 0.19331029, 0.22634371])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab_recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hybrid_Calculator():\n",
    "    \n",
    "    hybrid_interests = np.add(content_recommended,collab_recommendeds)\n",
    "    \n",
    "    similar_scores = cosine_similarity(hybrid_interests.reshape(1,10),Lda_articlemat)\n",
    "    \n",
    "    recommended_article_address =  np.argsort(similar_scores)[::-1]\n",
    "    \n",
    "    return recommended_article_address    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_recommend_indexes =  Hybrid_Calculator() #we get hyrid interest with our variations of 0.4 content based and 0.6 collab based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended-Articles :\n",
      "\n",
      "\n",
      "2778    Shashi Tharoor s Scalding Oxford Union Speech ...\n",
      "1470    Top 10 Technology Innovations at CES 2015  Wea...\n",
      "3274    Sensex slumps 250 points ahead of Bihar electi...\n",
      "1559    Rio Olympics 2016 tennis preview  Djokovic vs ...\n",
      "3418    Kashmir Conflict LIVE  Fresh Ceasefire Violati...\n",
      "3913    US may lift 40-year-old ban on exporting crude...\n",
      "2579    Human rights activists mount pressure on Indon...\n",
      "3304    Why is  Soft State  India Not Taking Action Ag...\n",
      "4369    India  US launch  ease of doing business  grou...\n",
      "1977    ATP World Tour Finals results  Federer tops gr...\n",
      "2811    Kashmir tense after Hizbul Mujahideen militant...\n",
      "4744    Indian Defence Minister Clears Deck for Acquis...\n",
      "1913    AIB Roast Controversy and Stringent Censor Boa...\n",
      "2604    China cites  large differences  over non-NPT s...\n",
      "2194    PVP  Dil Raju to Remake  Bangalore Days  with ...\n",
      "4468    Rupee closes at 68 30  near all-time low of 68...\n",
      "2357    Nobody knows the system better than me  I alon...\n",
      "2420    Gunman massacres 50 at Florida gay club in wor...\n",
      "4401    China goes for  23 13 billion infra push to be...\n",
      "3488      Sultan  star Salman Khan to ditch action films \n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for articles in hybrid_recommend_indexes :\n",
    "    \n",
    "    print('Recommended-Articles :')\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    print(title[articles][:no_of_recommends])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
