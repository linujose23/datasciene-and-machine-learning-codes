{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "import nltk\n",
    "import  sklearn\n",
    "from collections import Counter\n",
    "stemmer = SnowballStemmer('english')\n",
    "news = pd.read_csv('/home/linu/news_articles.csv')\n",
    "news.head()\n",
    "tokenizer = ToktokTokenizer()\n",
    "stops = set(stopwords.words('english'))\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "no_of_recommends = 5\n",
    "n_topics = 8\n",
    "\n",
    "articletime_second = 120\n",
    "articletime_seventh = 60\n",
    "articletime_hundredth = 30\n",
    "\n",
    "news = news[['Article_Id','Title','Content']].dropna()\n",
    "contents = news[\"Content\"].tolist()\n",
    "title = news['Title']\n",
    "article_id = news['Article_Id']\n",
    "\n",
    "def clean_tokenize(document):\n",
    "    document = re.sub('[^\\w_\\s-]',' ',document)\n",
    "    tokens  = nltk.word_tokenize(document)\n",
    "    cleaned_article = ' '.join([stemmer.stem(item) for item in tokens])   #stemming the tokenized corpus\n",
    "    return cleaned_article\n",
    "\n",
    "cleaned_articles = list(map(clean_tokenize,contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_vocab = { }\n",
    "\n",
    "article_vocab = enumerate(cleaned_articles)\n",
    "\n",
    "total_words = []\n",
    "\n",
    "for i in range(0, len(cleaned_articles)):\n",
    "    tokens = nltk.word_tokenize(cleaned_articles[i])\n",
    "\n",
    "    for w in tokens:\n",
    "        total_words.append(w)\n",
    "counts = Counter(total_words)\n",
    "\n",
    "vocab = {j:i for i,j in enumerate(counts.keys())}\n",
    "\n",
    "stops_removed = [i for i in vocab.keys() if i not in stops]\n",
    "\n",
    "final_vocab = {j:i for i,j in enumerate(stops_removed)}\n",
    "\n",
    "\n",
    "tf_idf = TfidfVectorizer(vocabulary=final_vocab,min_df=1)\n",
    "\n",
    "article_vocabulary = tf_idf.fit_transform(cleaned_articles)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics,max_iter=1,random_state=0)\n",
    "\n",
    "Lda_articlemat = lda.fit_transform(article_vocabulary)\n",
    "\n",
    "wordtokens_article = [word.split() for word in cleaned_articles]   #we tokenize each word in our article to divide by word per second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 227 325\n"
     ]
    }
   ],
   "source": [
    "print(len(wordtokens_article[0]),len(wordtokens_article[2]),len(wordtokens_article[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07380161 0.07372557 1.20671618 0.0737193  1.15962764 0.07373607\n",
      " 0.07371179 2.42932008]\n"
     ]
    }
   ],
   "source": [
    "# a function to create user profiles\n",
    "\n",
    "def user_profiler(wordtokens,article_read,article_time):\n",
    "    user_profile = []\n",
    "    wordPer_second = 5\n",
    "    #print(len(wordtokens_article))\n",
    "\n",
    "    for i in range(len(wordtokens)):                                        \n",
    "\n",
    "        average_time = (len(wordtokens[i])/wordPer_second) #length of wordtokslist by wps gives us avg time to read the article\n",
    "         \n",
    "        user_interest_timevalue = article_time[i]/average_time  #article_times divide by avg times of each article                   \n",
    "       \n",
    "        user_profile_generate = (article_read[i]*user_interest_timevalue)          #Ldamatrix[] * user_interest_time calculated                 \n",
    "        \n",
    "        user_profile.append(user_profile_generate)                                          \n",
    "\n",
    "    return sum(user_profile)\n",
    "\n",
    "\n",
    "userProfile_One = user_profiler([wordtokens_article[2],wordtokens_article[7],wordtokens_article[100]],\n",
    "                         [Lda_articlemat[2],Lda_articlemat[7],Lda_articlemat[100]],\n",
    "                         [120,60,30])\n",
    "\n",
    "userProfile_Two = user_profiler([wordtokens_article[9],wordtokens_article[500],wordtokens_article[300]],\n",
    "                         [Lda_articlemat[9],Lda_articlemat[500],Lda_articlemat[300]],\n",
    "                         [111,120,180])\n",
    "\n",
    "userProfile_Three = user_profiler([wordtokens_article[3],wordtokens_article[502],wordtokens_article[390]],\n",
    "                           [Lda_articlemat[3],Lda_articlemat[502],Lda_articlemat[390]],\n",
    "                           [200,120,100])\n",
    "\n",
    "\n",
    "userprofile_List = [userProfile_One,userProfile_Two,userProfile_Three]\n",
    "print(userProfile_One)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizer(norm) :\n",
    "    n = []\n",
    "\n",
    "    for profiles in userprofile_List:\n",
    "\n",
    "        user_preffered_articles = cosine_similarity(profiles.reshape(1,-1),Lda_articlemat)\n",
    "        a = np.argsort(user_preffered_articles).flatten()[::-1][:5]\n",
    "\n",
    "        n.append(a)\n",
    "\n",
    "    return n\n",
    "\n",
    "similarityscore = normalizer(userprofile_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2794, 1440, 1118, 3118,  140]),\n",
       " array([  88,  414, 2359, 1802, 1446]),\n",
       " array([3424, 4020, 4584, 3095, 4817])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarityscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#news['Title'][similarityscore[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Recommended Articles :\n",
      "\n",
      "\n",
      "2794    J K  8  Including 4 CRPF Jawans  Injured in An...\n",
      "1440     Pokemon Go  guide  List of every Pokemon with...\n",
      "1118    India replaces the US as world s second-bigges...\n",
      "3118    Nagaland  Mob Drags  Rapist  Out of Dimapur Ja...\n",
      "140     Nivin Pauly s Premam chosen as Apple Music s B...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommended Articles :\n",
      "\n",
      "\n",
      "88       Vijay s Theri to clash with Suriya s 24 in April\n",
      "414     Overseas box office collection   Kanche  conti...\n",
      "2359    Donald Trump is the latest villain M O D A A K...\n",
      "1802    Complete 2016 Badminton Schedule  Men s and Wo...\n",
      "1446    Micromax Yu Yuphoria  6 Reasons Why it May Rep...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommended Articles :\n",
      "\n",
      "\n",
      "3424    Delhi police bust Jaish-e-Mohammad terror cell...\n",
      "4020    Reliance to launch online fashion shopping por...\n",
      "4584    Lionel Messi is global brand ambassador of Tat...\n",
      "3095    Bali Nine Case  Indonesia President Widodo Off...\n",
      "4817    Six Essential Security Tips Every Whatsapp Use...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for i in similarityscore:\n",
    "    print('\\n')\n",
    "    print('Recommended Articles :')\n",
    "    print('\\n')\n",
    "    print(news['Title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Lda_articlemat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
